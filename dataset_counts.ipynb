{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f3b83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bAbI\n",
      "BBQ\n",
      "BLiMP\n",
      "BOLD\n",
      "BoolQ\n",
      "CivilComments\n",
      "APPS\n",
      "HumanEval\n",
      "HellaSwag\n",
      "OpenBookQA\n",
      "Copyright\n",
      "Copyright\n",
      "Copyright\n",
      "Copyright\n",
      "Copyright\n",
      "Disinformation - HELM\n",
      "DyckLanguage\n",
      "EntityDataImputation\n",
      "EntityMatching\n",
      "GSM8K\n",
      "ICE\n",
      "IMDB\n",
      "LegalSupport\n",
      "LSAT\n",
      "MATH\n",
      "MMLU\n",
      "MS MARCO\n",
      "NarrativeQA\n",
      "Natural Questions\n",
      "QuAC\n",
      "RAFT\n",
      "RealToxicityPrompts\n",
      "CNN/Daily Mail\n",
      "XSum\n",
      "SyntheticEfficiency\n",
      "SyntheticReasoning\n",
      "SynetheticReasoningNatural\n",
      "The Pile\n",
      "TruthfulQA\n",
      "TwitterAAE\n",
      "WikiFact\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from light_scenario import ScenarioSpecInstanceIds\n",
    "import cattrs\n",
    "mapping = {\n",
    "    \"BabiQA\": [\"bAbI\"],\n",
    "    \"BBQ\": [\"BBQ\"],\n",
    "    \"BLiMP\": [\"BLiMP\"],\n",
    "    \"BOLD\": [\"BOLD\"],\n",
    "    \"BoolQ\": [\"BoolQ\"],\n",
    "    \"CivilComments\": [\"CivilComments\"],\n",
    "    \"Code\": [\"HumanEval\", \"APPS\"],\n",
    "    'Copyright':['Copyright'],\n",
    "    \"CommonSense\": [\"CommonSenseQA\", \"HellaSwag\", \"PIQA\", \"SIQA\"], \n",
    "    \"Disinformation\": [\"Disinformation - HELM\"],\n",
    "    \"DyckLanguage\": [\"DyckLanguage\"],\n",
    "    \"EntityDataImputation\": [\"EntityDataImputation\"],\n",
    "    \"EntityMatching\": [\"EntityMatching\"],\n",
    "    \"GSM8K\": [\"GSM8K\"],\n",
    "    \"ICE\": [\"ICE\"],\n",
    "    \"IMDB\": [\"IMDB\"],\n",
    "    \"LegalSupport\": [\"LegalSupport\"],\n",
    "    \"LSAT\": [\"LSAT\"],\n",
    "    \"MATH\": [\"MATH\"],\n",
    "    \"MMLU\": [\"MMLU\"],\n",
    "    \"MSMARCO\": [\"MS MARCO\"],\n",
    "    \"NarrativeQA\": [\"NarrativeQA\"],\n",
    "    \"NaturalQA\": [\"Natural Questions\"],\n",
    "    \"QuAC\": [\"QuAC\"],\n",
    "    \"RAFT\": [\"RAFT\"],\n",
    "    \"RealToxicityPrompts\": [\"RealToxicityPrompts\"],\n",
    "    \"Summarization\": [\"XSum\", \"CNN/Daily Mail\"],\n",
    "    \"SyntheticEfficiency\": [\"SyntheticEfficiency\"],\n",
    "    \"SyntheticReasoning\": [\"SyntheticReasoning\"],\n",
    "    \"SRN\": [\"SynetheticReasoningNatural\"],\n",
    "    \"ThePile\": [\"The Pile\"],\n",
    "    \"TruthfulQA\": [\"TruthfulQA\"],\n",
    "    \"TwitterAAE\": [\"TwitterAAE\"],\n",
    "    \"WIKIFact\": [\"WikiFact\"]\n",
    "}\n",
    "dataset_mapping = {\n",
    "    \"humaneval\": \"HumanEval\",\n",
    "    \"apps\": \"APPS\",\n",
    "    \"xsum-sampled\": \"XSum\",\n",
    "    \"cnn-dm\": \"CNN/Daily Mail\",\n",
    "    'hellaswag': 'HellaSwag',\n",
    "    'openbookqa': 'OpenBookQA',\n",
    "}\n",
    "\n",
    "def scenario_spec_to_dataset_name(scenario_spec):\n",
    "    \"\"\" Get the dataset name from scenario_spec \"\"\"\n",
    "    class_name = scenario_spec.class_name.split('.')[-1][:-8]  # Get only the class name, not the full module path\n",
    "    key = mapping[class_name]\n",
    "    \n",
    "    args = scenario_spec.args  # ScenarioSpec args\n",
    "    \n",
    "    \n",
    "    if len(key) == 1:\n",
    "        return key[0]\n",
    "    \n",
    "    dataset = args['dataset_name'] if 'dataset_name' in args else args['dataset']\n",
    "    key = dataset_mapping[dataset]\n",
    "    return key\n",
    "\n",
    "class_name_to_counts = dict()\n",
    "scenario_spec_instance_id_dict = dict()\n",
    "scenario_spec_instance_ids_json = 'filtered_scenario_spec_instance_ids.json'\n",
    "scenario_spec_instance_ids_jsons = open(scenario_spec_instance_ids_json, \"r\").readlines()\n",
    "for scenario_spec_instance_ids_json in scenario_spec_instance_ids_jsons:\n",
    "    scenario_spec_instance_ids_dict = json.loads(scenario_spec_instance_ids_json)\n",
    "    scenario_spec_instance_ids = cattrs.structure(scenario_spec_instance_ids_dict, ScenarioSpecInstanceIds)\n",
    "    scenario_spec_instance_id_dict[\n",
    "        scenario_spec_instance_ids.scenario_spec\n",
    "    ] = scenario_spec_instance_ids.instance_ids\n",
    "    class_name = scenario_spec_to_dataset_name(scenario_spec_instance_ids.scenario_spec)\n",
    "    if class_name not in class_name_to_counts:\n",
    "        print(class_name)\n",
    "        if class_name == 'Copyright':\n",
    "            continue\n",
    "#         print(scenario_spec_instance_ids_json)\n",
    "        class_name_to_counts[class_name] = 0\n",
    "    class_name_to_counts[class_name] += len(scenario_spec_instance_ids.instance_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e0d05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bAbI': 4000,\n",
       " 'BBQ': 1000,\n",
       " 'BLiMP': 4000,\n",
       " 'BOLD': 1000,\n",
       " 'BoolQ': 5128,\n",
       " 'CivilComments': 45000,\n",
       " 'APPS': 1000,\n",
       " 'HumanEval': 164,\n",
       " 'HellaSwag': 1000,\n",
       " 'OpenBookQA': 500,\n",
       " 'Disinformation - HELM': 79,\n",
       " 'DyckLanguage': 500,\n",
       " 'EntityDataImputation': 424,\n",
       " 'EntityMatching': 1400,\n",
       " 'GSM8K': 1000,\n",
       " 'ICE': 2939,\n",
       " 'IMDB': 4421,\n",
       " 'LegalSupport': 1000,\n",
       " 'LSAT': 461,\n",
       " 'MATH': 874,\n",
       " 'MMLU': 1641,\n",
       " 'MS MARCO': 2504,\n",
       " 'NarrativeQA': 2350,\n",
       " 'Natural Questions': 7876,\n",
       " 'QuAC': 4321,\n",
       " 'RAFT': 1348,\n",
       " 'RealToxicityPrompts': 1000,\n",
       " 'CNN/Daily Mail': 1000,\n",
       " 'XSum': 1000,\n",
       " 'SyntheticEfficiency': 600,\n",
       " 'SyntheticReasoning': 3000,\n",
       " 'SynetheticReasoningNatural': 2000,\n",
       " 'The Pile': 2952,\n",
       " 'TruthfulQA': 1895,\n",
       " 'TwitterAAE': 2000,\n",
       " 'WikiFact': 7927}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name_to_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/{dataset}_latex_table.txt', 'w') as output_file:\n",
    "    for index, row in sorted_df.iterrows():\n",
    "        formatted_output = f\"{row['Scenario']} & {row['Part']} & {row['Count']} & {format(row['Binary'], '.4f')} & {format(row['Jaccard'], '.4f')} & {format(row['Token'], '.4f')} & {format(row['Filtered Binary'], '.4f')} & {format(row['Filtered Jaccard'], '.4f')} & {format(row['Filtered Token'], '.4f')} & {format(row['Weighted Jaccard'], '.4f')} & {format(row['Weighted Token'], '.4f')} & {format(row['Weighted Filtered Jaccard'], '.4f')} & {format(row['Weighted Filtered Token'], '.4f')} \\\\\\\\\\n\"\n",
    "#         print(formatted_output)\n",
    "        output_file.write(formatted_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
